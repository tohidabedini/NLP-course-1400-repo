{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# references\n",
    "# https://networking.ringofsaturn.com/Web/removetags.php\n",
    "# https://towardsdatascience.com/5-simple-ways-to-tokenize-text-in-python-92c6804edfc4\n",
    "# https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "# https://www.geeksforgeeks.org/python-lemmatization-with-nltk/\n",
    "# https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas\n",
    "# https://machinelearningmastery.com/train-test-split-for-evaluating-machine-learning-algorithms/\n",
    "# https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "# https://towardsdatascience.com/multi-class-metrics-made-simple-part-i-precision-and-recall-9250280bddc2\n",
    "# https://androidkt.com/micro-macro-averages-for-imbalance-multiclass-classification/\n",
    "# https://towardsdatascience.com/multi-class-metrics-made-simple-part-ii-the-f1-score-ebe8b2c2ca1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3v9p59dC-5y"
   },
   "source": [
    "# Intro\n",
    "In the Name of Allah\n",
    "\n",
    "Sentiment analysis is a technique through which you can analyze a piece of text to determine the sentiment behind it. In this notebook, we're going to train a Naïve Bayes Classifier for the task of sentiment analysis on Imdb movie reviews dataset.\n",
    "\n",
    "**Please pay attention to these notes:**\n",
    "\n",
    "<br/>\n",
    "\n",
    "- **Assignment Due:** 1400/09/19 23:59\n",
    "- Write your code in the cells denoted by:\n",
    "```\n",
    "######## Your Code Here ########\n",
    "```\n",
    "- You can add more cells if necessary\n",
    "- Finding any sort of copying will zero down your grade.\n",
    "- When your solution is ready to submit, don't forget to set the name of this notebook like  \"Name_StudentID.ipynb\".\n",
    "- If you have any questions about this assignment, feel free to drop us a line. You can also ask your questions on the telegram group.\n",
    "- You must run this notebook on Google Colab platform.\n",
    "\n",
    "<br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iryuq7HoLy3j"
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "W-R7b4iAuKnP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Tohid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Tohid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Tohid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# importing the libraries\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import collections\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split as tts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Cj9uxuvswOpA"
   },
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/Ankit152/IMDB-sentiment-analysis/master/IMDB-Dataset.csv\n",
    "# I ran code on my laptop with Windows which does not have wget (can not easily used) so I commented this cell and used curl in git-bash to download it.\n",
    "# !curl https://raw.githubusercontent.com/Ankit152/IMDB-sentiment-analysis/master/IMDB-Dataset.csv > IMDB-Dataset.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqW0otKAQWB9"
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "v4ujxrR8QkeU"
   },
   "outputs": [],
   "source": [
    "imdb = pd.read_csv(\"IMDB-Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "370oVEkqe8mp"
   },
   "source": [
    "# Preprocess\n",
    "The first step of NLP is text preprocessing. Data cleaning is a very crucial step in any machine learning model, but more so for NLP. Without the cleaning process, the dataset is often a cluster of words that the computer doesn’t understand. Raw data over a properly or improperly formed sentence is not always desirable as it contains lot of unwanted components like null/html/links/url/emoji/stopwords etc. So in this step, this unwanted components are removed for better performance and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: html tags removing\n",
    "def html_tag_remover(text):\n",
    "    text = re.sub('<[a-zA-Z\\/][^>]*>', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# step 2: punctuations removing\n",
    "def punctuations_remover(text):\n",
    "    text = re.sub('[^\\w\\s\\d]', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# step 3: lowercasing\n",
    "def lowercaser(text):\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "# step 4: tokenizing\n",
    "def tokenizer(text):\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# step 5: stopwords removing\n",
    "def stopwords_remover(tokens):\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    stop_words_removed_list = []\n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            stop_words_removed_list.append(token)\n",
    "            \n",
    "    return stop_words_removed_list\n",
    "\n",
    "\n",
    "# step 6: lemmatizing\n",
    "def lemmatizer(tokens):\n",
    "    lemmatized_tokens = []\n",
    "    for token in tokens:\n",
    "        lemmatized_tokens.append(WordNetLemmatizer().lemmatize(token))\n",
    "    \n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put it all together\n",
    "def text_preprocessor(text):\n",
    "    html_tag_removed = html_tag_remover(text)\n",
    "    punctuations_removed = punctuations_remover(html_tag_removed)\n",
    "    lowercased = lowercaser(punctuations_removed)\n",
    "    tokens = tokenizer(lowercased)\n",
    "    stopwords_removed_tokens = stopwords_remover(tokens)\n",
    "    lemmatized_stopwords_removed_tokens = lemmatizer(stopwords_removed_tokens)\n",
    "\n",
    "    return list(set(lemmatized_stopwords_removed_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "# test_sample = imdb.iloc[1][\"review\"]\n",
    "# html_tag_remover(test_sample)\n",
    "# punctuations_remover(test_sample)\n",
    "# lowercaser(test_sample)\n",
    "# tokens = tokenizer(test_sample)\n",
    "# tokens = stopwords_remover(tokens)\n",
    "# lemmatizer(tokens)\n",
    "# test_sample\n",
    "\n",
    "# lemmatizer(stopwords_remover(tokenizer(lowercaser(punctuations_remover(html_tag_remover(test_sample))))))\n",
    "\n",
    "# text_preprocessor(test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on dataset and make a list of preprocessed tokens and labels\n",
    "def dataset_preprocess(df):\n",
    "    dataset_preprocessed_list = []\n",
    "    labels = []\n",
    "    for index, row in df.iterrows():\n",
    "        if index%10000==0:\n",
    "            print(index)\n",
    "        dataset_preprocessed_list.append(text_preprocessor(row[\"review\"]))\n",
    "        labels.append(row[\"sentiment\"])\n",
    "    return dataset_preprocessed_list, labels\n",
    "\n",
    "def labels_encoder(labels):\n",
    "    encoded_labels = []\n",
    "    for i in labels:\n",
    "        if i == \"positive\":\n",
    "            encoded_labels.append(1)\n",
    "        elif i == \"negative\":\n",
    "            encoded_labels.append(0)\n",
    "            \n",
    "    return encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n"
     ]
    }
   ],
   "source": [
    "dataset, labels = dataset_preprocess(imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_labels = labels_encoder(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "['side', 'street', 'em', 'scene', 'reviewer', 'mainly', 'get', 'pretty', 'audience', 'wholl', 'mentioned', 'prison', 'experimental', 'inwards', 'section', 'middle', 'timid', 'classic', 'sold', 'first', 'viewingthats', 'called', 'punch', 'home', 'dealing', 'become', 'use', 'got', 'show', 'awayi', 'pull', 'manyaryans', 'faint', 'hooked', 'taste', 'developed', 'never', 'nickname', 'due', 'nasty', 'say', 'front', 'hearted', 'muslim', 'may', 'glass', 'surreal', 'mess', 'touch', 'uncomfortable', 'face', 'away', 'happened', 'painted', 'romanceoz', 'episode', 'death', 'agenda', 'charm', 'one', 'experience', 'youll', 'regard', 'set', 'comfortable', 'mannered', 'skill', 'crooked', 'graphic', 'inmate', 'watching', 'hardcore', 'go', 'security', 'doesnt', 'wouldnt', 'city', 'turned', 'class', 'penitentary', 'latino', 'emerald', 'dodgy', 'sex', 'fact', 'dare', 'italian', 'moreso', 'guard', 'bitch', 'exactly', 'brutality', 'drug', 'wordit', 'forget', 'unflinching', 'injustice', 'word', 'gangsta', 'mainstream', 'violence', 'trust', 'high', 'scuffle', 'shady', 'watched', 'would', 'accustomed', 'methe', 'ever', 'well', 'level', 'christian', 'struck', 'irish', 'agreement', 'picture', 'nickel', 'right', 'cell', 'around', 'given', 'maximum', 'saw', 'far', 'order', 'kill', '1', 'stare', 'focus', 'oswald', 'darker', 'couldnt', 'oz', 'state', 'lack', 'main', 'thing', 'ready', 'privacy', 'appeal']\n"
     ]
    }
   ],
   "source": [
    "print(len(encoded_labels))\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qeB0RNlJ89_g"
   },
   "source": [
    "<font size=\"5\">Split the dataset</font>\n",
    "\n",
    "Data splitting, or commonly known as train-test split, is the partitioning of data into subsets for model training and evaluation separately. Since the test set is not specified beforehand, we have to split the dataset into train and test set in an ideal proportion. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "hZT57Dblyf8Y"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = tts(dataset, encoded_labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n",
      "10000\n",
      "40000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(X_test))\n",
    "print(len(y_train))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnesufkigiPw"
   },
   "source": [
    "# Training\n",
    "Use Naive Beyes algorithm to train a Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NB_classifier:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "         \n",
    "    # compute classes prior probability\n",
    "    def compute_classes_probability(self):\n",
    "        classes_probability = np.zeros(self.no_classes)\n",
    "        for i in range(len(classes_probability)):\n",
    "            classes_probability[i] = self.train_labels.count(i) / self.train_len\n",
    "            \n",
    "        return classes_probability\n",
    "            \n",
    "        \n",
    "    # compute dictionaries of words occurrence for each class \n",
    "    def compute_dictionaries(self):\n",
    "        all_classes_dictionaries = [{} for i in range(self.no_classes)]\n",
    "        for row in range(len(self.train_set)):\n",
    "            for token in self.train_set[row]:\n",
    "                if token in all_classes_dictionaries[self.train_labels[row]]:\n",
    "                    all_classes_dictionaries[self.train_labels[row]][token]+=1\n",
    "                else:\n",
    "                    all_classes_dictionaries[self.train_labels[row]][token]=1\n",
    "            \n",
    "        return all_classes_dictionaries\n",
    "               \n",
    "    # computer vocab size\n",
    "    def computer_vocab_size(self):\n",
    "        all_words = list()\n",
    "        for i in range(self.no_classes):\n",
    "            all_words+=list(set(list(self.all_classes_dictionaries[i].keys())))\n",
    "        \n",
    "        vocab_size = len(set(all_words))\n",
    "        \n",
    "        return vocab_size\n",
    "    \n",
    "    # count all words in each class\n",
    "    def count_classes_words(self):\n",
    "        classes_words_count = np.zeros(self.no_classes)\n",
    "        for i in range(self.no_classes):\n",
    "            classes_words_count[i]= sum(self.all_classes_dictionaries[i].values())\n",
    "            \n",
    "        return classes_words_count\n",
    "    \n",
    "    \n",
    "    # token probablity calculator with - log probability is used to prevent underflow\n",
    "    def token_probability_calculator(self, token, class_no):\n",
    "        if token in self.all_classes_dictionaries[class_no]:\n",
    "            count = self.all_classes_dictionaries[class_no][token]\n",
    "        else:\n",
    "            count = 0 \n",
    "        probability = (count + 1) / (self.vocab_size + self.classes_words_count[class_no])\n",
    "        \n",
    "        return np.log(probability)\n",
    "        \n",
    "        \n",
    "       \n",
    "    # train - put it all together\n",
    "    def train(self, train_set, train_labels):\n",
    "        print(\"Training: computing probabilities and required info\")\n",
    "        self.train_set = train_set\n",
    "        self.train_labels = train_labels\n",
    "        self.train_len = len(self.train_set)\n",
    "        self.no_classes = len(set(self.train_labels))\n",
    "        self.classes_probability = self.compute_classes_probability()\n",
    "        \n",
    "        self.all_classes_dictionaries = self.compute_dictionaries()\n",
    "        self.vocab_size = self.computer_vocab_size()\n",
    "        self.classes_words_count = self.count_classes_words()\n",
    "        \n",
    "\n",
    "    # compute probability for each class and return the max one as  the predicted one\n",
    "    # mode==0: input is tokenized, mode==1: input is raw text\n",
    "    # log==True: print the probabilities\n",
    "    def predict_a_sentence_probability(self, sentence, mode=0, log=False):\n",
    "        probabilities = np.zeros(self.no_classes)\n",
    "        if mode==0:\n",
    "            tokens = sentence\n",
    "        elif mode==1:\n",
    "            tokens = text_preprocessor(sentence)\n",
    "            \n",
    "        for i in range(self.no_classes):\n",
    "            partial_probabilities = np.zeros(len(tokens)+1)\n",
    "            for j in range(len(tokens)):\n",
    "                partial_probabilities[j] = self.token_probability_calculator(tokens[j], i)\n",
    "                \n",
    "            partial_probabilities[-1] = np.log(self.classes_probability[i])\n",
    "            probabilities[i] = np.sum(partial_probabilities)\n",
    "            \n",
    "        if log:\n",
    "            print(probabilities)\n",
    "        \n",
    "        return np.argmax(probabilities)\n",
    "    \n",
    "    # test on all samples of a dataset \n",
    "    # mode==0: input is tokenized, mode==1: input is raw text\n",
    "    # log==True: print the probabilities\n",
    "    def evaluate_on_a_dataset(self, test_set, mode=0, log=False):\n",
    "        predicted_classes = np.zeros(len(test_set))\n",
    "        for i in range(len(test_set)):\n",
    "            predict = self.predict_a_sentence_probability(test_set[i], mode=mode, log=log)\n",
    "            predicted_classes[i] = predict\n",
    "        return predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: computing probabilities and required info\n"
     ]
    }
   ],
   "source": [
    "NB = NB_classifier()\n",
    "NB.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB.no_classes:  2\n",
      "NB.classes_probability:  [0.500975 0.499025]\n",
      "NB.vocab_size:  181839\n",
      "NB.classes_words_count:  [1949003. 1966939.]\n"
     ]
    }
   ],
   "source": [
    "print(\"NB.no_classes: \",NB.no_classes)\n",
    "print(\"NB.classes_probability: \",NB.classes_probability)\n",
    "# print(NB.all_classes_dictionaries[0])\n",
    "print(\"NB.vocab_size: \",NB.vocab_size)\n",
    "print(\"NB.classes_words_count: \",NB.classes_words_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvSqU886lMDN"
   },
   "source": [
    "# Test\n",
    "Now you need to run inference on your test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "0Pr2rqDwlPpU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1477.70301043 -1448.72599749]\n",
      "1\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# one sample\n",
    "print(NB.predict_a_sentence_probability(X_test[999],log=True))\n",
    "print(\"-------------------------------------\")\n",
    "# all dataset\n",
    "y_hat_test = NB.evaluate_on_a_dataset(X_test, mode=0, log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "[1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(len(y_hat_test))\n",
    "print(y_hat_test[:20])\n",
    "# 1 is positive - 0 is negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBS9HFA8iWlX"
   },
   "source": [
    "# Evaluation\n",
    "After training is finished, we need some metrics to evaluate the trained model on the test set. Here, you need to write code for utilizing the metrics bellow without the sklearn libraries!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIeLqza_llil"
   },
   "source": [
    "Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "id": "tjzeJiZWloV8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn-acc      : 0.8537\n",
      "sklearn-presicion: 0.8726552730304293\n",
      "sklearn-recall   : 0.8309188331018059\n",
      "sklearn-f1       : 0.8512757954660976\n",
      "sklearn-confusion matric:\n",
      " [[4350  611]\n",
      " [ 852 4187]]\n"
     ]
    }
   ],
   "source": [
    "# note that the imports from sklearn are only for showing that the results are correct!\n",
    "from sklearn import metrics\n",
    "print(\"sklearn-acc      :\",metrics.accuracy_score(y_test, y_hat_test))\n",
    "print(\"sklearn-presicion:\",metrics.precision_score(y_test, y_hat_test))\n",
    "print(\"sklearn-recall   :\",metrics.recall_score(y_test, y_hat_test))\n",
    "print(\"sklearn-f1       :\",metrics.f1_score(y_test, y_hat_test))\n",
    "print(\"sklearn-confusion matric:\\n\",metrics.confusion_matrix(y_test, y_hat_test))\n",
    "\n",
    "\n",
    "# calculate confusion matrix by comparing the predicted results and real values\n",
    "def confusion_matrix_calculator(y_test, y_pred, no_classes=2):\n",
    "    assert len(y_test)==len(y_pred), \"Lengths are not equal!\"\n",
    "    if no_classes==None:\n",
    "        no_classes = len(set(y_test))\n",
    "\n",
    "    confusion_matrix = np.zeros((no_classes,no_classes))\n",
    "    \n",
    "    for i in range(len(y_test)):\n",
    "        confusion_matrix[int(y_test[i]), int(y_pred[i])] +=1\n",
    "            \n",
    "    return confusion_matrix\n",
    "\n",
    "\n",
    "def precision_calculator(confusion_matrix):\n",
    "    # precision = tp/(tp+fp)\n",
    "    tp = confusion_matrix[1,1]\n",
    "    fp = confusion_matrix[0,1]\n",
    "    precision = tp/(tp+fp)\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:  0.8726552730304293\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix = confusion_matrix_calculator(y_test, y_hat_test, no_classes=2)\n",
    "precision = precision_calculator(confusion_matrix)\n",
    "print(\"precision: \", precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Go2HcCbTln_M"
   },
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "id": "phSzQZ6JlsV1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall:  0.8309188331018059\n"
     ]
    }
   ],
   "source": [
    "def recall_calculator(confusion_matrix):\n",
    "    # recall = tp/(tp+fn)\n",
    "    tp = confusion_matrix[1,1]\n",
    "    fn = confusion_matrix[1,0]\n",
    "    recall = tp/(tp+fn)\n",
    "    return recall\n",
    "\n",
    "recall = recall_calculator(confusion_matrix)\n",
    "print(\"recall: \", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUM6OQNAlqks"
   },
   "source": [
    "F-measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "id": "EnzV0G1tlsA8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:  0.8512757954660976\n"
     ]
    }
   ],
   "source": [
    "def f1_measure_calculator(confusion_matrix):\n",
    "    # f1 = 2 * (precision*recall) / (precision+recall)\n",
    "    precision = precision_calculator(confusion_matrix)    \n",
    "    recall    = recall_calculator(confusion_matrix)\n",
    "    f1 = 2 * (precision*recall) / (precision+recall)\n",
    "    return f1\n",
    "\n",
    "f1_score = f1_measure_calculator(confusion_matrix)\n",
    "print(\"f1_score: \", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmo7-ZsEytmo"
   },
   "source": [
    "Confustion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "id": "sVFCeotJy1DZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4350.,  611.],\n",
       "       [ 852., 4187.]])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Bayes_Classifier_Notebook_test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
