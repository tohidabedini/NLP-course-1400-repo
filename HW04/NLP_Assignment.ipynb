{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# https://keras.io/api/layers/core_layers/embedding/\n",
        "# https://stats.stackexchange.com/questions/326065/cross-entropy-vs-sparse-cross-entropy-when-to-use-one-over-the-other"
      ],
      "metadata": {
        "id": "t4_hTXjxnyhR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIXxF8oFp_85"
      },
      "source": [
        "# Character-level Generation with Sequential Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2koofrWqdGa"
      },
      "source": [
        "We have two kinds of text generation:\n",
        "\n",
        "\n",
        "1.   Character-level\n",
        "2.   Word Level\n",
        "\n",
        "In this assignment we are focusing on the first one. Using a sequence of characters, we are going to train a model to predict the next character in the sequence. We will run you through the process step by step.\n",
        "\n",
        "First, you have to import some required packages by running the cell below.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "H4aXGPoUp7V5"
      },
      "outputs": [],
      "source": [
        "#@title Import Requireed Packages\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iR_3GoGkrn32"
      },
      "source": [
        "Now it's time for the our data. In this assignment we will be using the \"Shakespeare\" data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mygHErGws5N4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc085833-89e6-4430-e554-d583139cf1d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n",
            "1130496/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "# Read and decode\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_0a_Uv-s74b"
      },
      "source": [
        "Let's take a look at the data (the first 250 characters)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "p5VLrwVgtBCX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a662859-c770-43dd-f9f6-03943c7fe3cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(text[:250])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJ5R4vfa4bVI"
      },
      "source": [
        "We now need to build up the vocabulary by finding the unique characters:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vocab"
      ],
      "metadata": {
        "id": "REg9ba1QOyrd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WRLgYlkB4a96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01fc58fd-6cb9-41ab-acc1-4e93158c1a57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ],
      "source": [
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcwAxyW0s6sS"
      },
      "source": [
        "## Preprocessing the text\n",
        "\n",
        "We now need to convert these strings to numerical representations so that our model can understand them. To do this you will need to use:\n",
        "\n",
        "```\n",
        "tf.keras.layers.StringLookup\n",
        "```\n",
        "and then pass it the vocabulary we created in the previous part.\n",
        "\n",
        "However, the text needs to be tokenized first.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Kh3hyAHW5VIh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b541ff1f-9e5c-41f8-8f7c-3391df3851b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tf.RaggedTensor [[50, 44, 44, 55], [58, 59, 54, 57, 44]]>\n"
          ]
        }
      ],
      "source": [
        "sample_text = ['keep', 'store']\n",
        "\n",
        "chars = tf.strings.unicode_split(sample_text, input_encoding='UTF-8')\n",
        "\n",
        "char2id = tf.keras.layers.StringLookup(vocabulary=vocab)\n",
        "\n",
        "\n",
        "ids = char2id(chars)\n",
        "\n",
        "print(ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A01oJaW36ZIH"
      },
      "source": [
        "Since our goal is to generate text and not just understand text, we need to convert machine understandable numeric data to human-understandable text. Therefore, we will need a method to convert these ids back to string.\n",
        "\n",
        "Use the same method you used for converting chars into id but this time use the option <font color='red'>invert=True</font>. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2MPsltu5DqJf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f9a29a9-e414-4c8f-add3-ec368c900a9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tf.RaggedTensor [[b'k', b'e', b'e', b'p'], [b's', b't', b'o', b'r', b'e']]>\n",
            "tf.Tensor([b'keep' b'store'], shape=(2,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "id2char = tf.keras.layers.StringLookup(vocabulary=vocab,  invert=True)\n",
        "\n",
        "chars = id2char(ids)\n",
        "print(chars)\n",
        "\n",
        "# now we want to use a code to join chars into strings\n",
        "# Hint: You should use a method from tf.strings called reduce_join\n",
        "def id2text(ids):\n",
        "    chars = id2char(ids)\n",
        "    text = tf.strings.reduce_join(chars, axis=-1)\n",
        "    return text\n",
        "\n",
        "print(id2text(ids))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eq095wplFrok"
      },
      "source": [
        "## Creating the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v412nM1wFxYT"
      },
      "source": [
        "In this section we would like to divide our text into sequences. Each sequence will be constrained by <font color='red'>sequence_length</font> that we define.\n",
        "\n",
        "The corresponding target sequence of each input sequence has the same length except one character shifted to the right. So you should break the text into chunks of <font color='red'> seq_length+1</font> For instance, given a seq_length of 5 and \"Python\" as text, the input sequence would be \"pytho\" the target would be \"ython\".\n",
        "\n",
        "To do this you should use:\n",
        "```\n",
        "tf.data.Dataset.from_tensor_slices\n",
        "```\n",
        "to convert the text vector into a sequence of character indices.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YrlxMdT_HkUM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a3ac6d5-8713-434d-a6cb-7a971a1faeb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([19 48 57 ... 46  9  1], shape=(1115394,), dtype=int64)\n",
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ],
      "source": [
        "all_ids = char2id(tf.strings.unicode_split(text, input_encoding='UTF-8'))\n",
        "print(all_ids)\n",
        "\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "\n",
        "\n",
        "for ids in ids_dataset.take(10):\n",
        "    print(id2char(ids).numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "N9lToSqBVRd0"
      },
      "outputs": [],
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XulJjvGlVS13"
      },
      "source": [
        "Now use the <font color='red'>batch</font> method to convert these characters to sequences with the desired length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "AG4-tH8IVnKj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "698faf6a-0885-401c-bf89-11cb150bf0dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n",
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ],
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(id2char(seq))\n",
        "  \n",
        "# Changing tokens back to text\n",
        "for seq in sequences.take(5):\n",
        "  print(id2text(seq).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtSr10QhV47C"
      },
      "source": [
        "To train your model you need a dataset consisting pairs of (input, label), where input and label are sequences. Given each time step, an input is the current character and the label is the next character. Now write a function that takes a sequence input, duplicates, and shifts it to align the input and label for each timestep:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "x1zQmHlrXLgS"
      },
      "outputs": [],
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUlKbDkJXWz1"
      },
      "source": [
        "example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "k70Jit5ZXYIH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "640cbef7-359a-4566-cef7-47b277261734"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(['H', 'e', 'l', 'l'], ['e', 'l', 'l', 'o'])\n"
          ]
        }
      ],
      "source": [
        "print(split_input_target(list(\"Hello\")))\n",
        "\n",
        "# Should return:\n",
        "''' ([H, e, l, l],\n",
        "    [e, l, l, o]) ''';"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "v8s6rg7jYBGH"
      },
      "outputs": [],
      "source": [
        "dataset_ = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "PCJJu2nWYSzm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff52a23f-c60e-4d03-a698-e89c20226599"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ],
      "source": [
        "for input_example, target_example in dataset_.take(1):\n",
        "    print(\"Input :\", id2text(input_example).numpy())\n",
        "    print(\"Target:\", id2text(target_example).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca7Lmz_ZZKuS"
      },
      "source": [
        "## Creating training batches\n",
        "\n",
        "Now you should shuffle the data and pack it into batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "f1X8Jcd_ZsQQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81db3e81-d1d4-43fb-9987-d90077c4eaa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size is used for shuffling the dataset\n",
        "BUFFER_SIZE = 1000\n",
        "\n",
        "# '''Your code for shuffling and batching the data also use .prefetch(tf.data.experimental.AUTOTUNE)) at the end'''\n",
        "dataset = dataset_.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for input_example, target_example in dataset_.take(1):\n",
        "#     print(\"Input :\", id2text(input_example).numpy())\n",
        "#     print(\"Target:\", id2text(target_example).numpy())"
      ],
      "metadata": {
        "id": "HYERSCB-cew4"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnDs0kdaabNl"
      },
      "source": [
        "## Building the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dslxli5ff1E5"
      },
      "source": [
        "Here you should build your model. Please use the following for your model:\n",
        "\n",
        "\n",
        "*   An embedding layer\n",
        "*   An RNN layer (LSTM or GRU)\n",
        "*   Dense Layer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "xRmxCjA5fzxo"
      },
      "outputs": [],
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "vocab_size_ = len(char2id.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "mQw5veIhgZjO"
      },
      "outputs": [],
      "source": [
        "class GenModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "        super().__init__(self)\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = tf.keras.layers.GRU(rnn_units,return_sequences=True,return_state=True)\n",
        "        self.rnn2 = tf.keras.layers.GRU(rnn_units,return_sequences=True,return_state=True) \n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "        # self.dropout = tf.keras.layers.Dropout(0.1) # I also used LSTM but it didn't help! -   less performance than GTU\n",
        "\n",
        "\n",
        "    def call(self, inputs, states_1=None, states_2=None, return_state=False, training=False):\n",
        "        x = inputs\n",
        "        '''pass the inputs through the embedding layer, the RNN layer, and then the dense layer. You should also check for initial states '''\n",
        "\n",
        "        x = self.embedding(x, training=training)\n",
        "        \n",
        "        if states_1 is None:\n",
        "            states_1 = self.rnn.get_initial_state(x)\n",
        "\n",
        "        if states_2 is None:\n",
        "            states_2 = self.rnn2.get_initial_state(x)\n",
        "\n",
        "        # x, memory_state, carry_state = self.rnn(x, initial_state=states, training=training)\n",
        "        x, states_1 = self.rnn(x, initial_state=states_1, training=training)\n",
        "        x, states_2 = self.rnn2(x, initial_state=states_2, training=training)\n",
        "\n",
        "        # x, states = self.rnn2(x, initial_state=states, training=training)\n",
        "        # x = self.dropout(x, training=training)\n",
        "        # states = [memory_state, carry_state]\n",
        "        x = self.dense(x, training=training)\n",
        "\n",
        "        if return_state:\n",
        "            return x,  states_1, states_2\n",
        "        else:\n",
        "            return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "w-gWS1B1hXRL"
      },
      "outputs": [],
      "source": [
        "model = GenModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=vocab_size_,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRhnsajDhrH6"
      },
      "source": [
        "## Check the model\n",
        "\n",
        "Now let's check our model to see if it behaves as expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "cOn-xz2Khyfr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e90352f-3f89-449c-d652-a9f051e38703"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "EwzxiHCkh-md",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00a5d057-67af-4b9c-8e51-bf44a4d5c662"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[42 23 35 56 56 61 21 40 64 42 63 17 30 53 57 25 55 21 21  2 41 12 28  5\n",
            " 48 53 35 53 14 10  1 37 47 57  0 53 34 37  4  1 46 13  5 19 33  8 27 57\n",
            " 34 40 24 34 38 53 51  7 28 46 48  5 63 44 37 39 10 40 10 13 51 11 52 40\n",
            " 27  3 30 23 45 58 36 33 23  1 41 29 22 25 43 34  4 57 13  6  7 61 54 64\n",
            "  0 64 29 63]\n"
          ]
        }
      ],
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "\n",
        "print(sampled_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "KHJyMJ3SiEnq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da6080d4-cd02-435f-eb94-3bdbce5448a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b'r them.\\n\\nSICINIUS:\\nThis, as you say, suggested\\nAt some time when his soaring insolence\\nShall touch t'\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"cJVqqvHaycxDQnrLpHH b;O&inVnA3\\nXhr[UNK]nUX$\\ng?&FT-NrUaKUYnl,Ogi&xeXZ3a3?l:maN!QJfsWTJ\\nbPILdU$r?',voy[UNK]yPx\"\n"
          ]
        }
      ],
      "source": [
        "print(\"Input:\\n\", id2text(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", id2text(sampled_indices).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTjxCVnwiL_G"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "We have now built our model. The only part left is to train and then test the model.\n",
        "\n",
        "We need a loss function. Please choose the correct loss function from the followings:\n",
        "\n",
        "\n",
        "*   Categorical Cross Entropy\n",
        "*   Sparse Categorical Cross Entropy\n",
        "*   Binary Cross Entropy\n",
        "*   MSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "l0h1o-1bjIS8"
      },
      "outputs": [],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "fcBTEqIYjM2J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6befd92-0ad6-42b8-c0fc-f5ee9464f899"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         4.1906624\n"
          ]
        }
      ],
      "source": [
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "mean_loss = example_batch_loss.numpy().mean()\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", mean_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB8PxGcjjZK8"
      },
      "source": [
        "Mean loss with no training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "rqf_G47XjeEE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bd7a510-ff31-4eef-a98f-156bfe1e1a8f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66.066536"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "tf.exp(mean_loss).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAVtH6tkjep3"
      },
      "source": [
        "Compile the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "i5ypuFc7joKN"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "8pkL7vAZh3tU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7948d7af-6d2f-4080-b9c1-09a4908232d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"gen_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " gru_1 (GRU)                 multiple                  6297600   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 10,320,450\n",
            "Trainable params: 10,320,450\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaqY2LBYjhpe"
      },
      "source": [
        "Configure Checkpoints:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Gx-ALXaZjmFM"
      },
      "outputs": [],
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "NkfhD3uPjyrL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbe27919-762e-4eb3-865a-630578cc4f71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "172/172 [==============================] - 70s 388ms/step - loss: 2.5554\n",
            "Epoch 2/30\n",
            "172/172 [==============================] - 67s 390ms/step - loss: 1.7953\n",
            "Epoch 3/30\n",
            "172/172 [==============================] - 67s 389ms/step - loss: 1.5409\n",
            "Epoch 4/30\n",
            "172/172 [==============================] - 67s 390ms/step - loss: 1.4154\n",
            "Epoch 5/30\n",
            "172/172 [==============================] - 67s 390ms/step - loss: 1.3415\n",
            "Epoch 6/30\n",
            "172/172 [==============================] - 67s 390ms/step - loss: 1.2854\n",
            "Epoch 7/30\n",
            "172/172 [==============================] - 67s 390ms/step - loss: 1.2387\n",
            "Epoch 8/30\n",
            "172/172 [==============================] - 67s 389ms/step - loss: 1.1917\n",
            "Epoch 9/30\n",
            "172/172 [==============================] - 67s 390ms/step - loss: 1.1444\n",
            "Epoch 10/30\n",
            "172/172 [==============================] - 67s 389ms/step - loss: 1.0923\n",
            "Epoch 11/30\n",
            "172/172 [==============================] - 67s 388ms/step - loss: 1.0364\n",
            "Epoch 12/30\n",
            "172/172 [==============================] - 67s 389ms/step - loss: 0.9725\n",
            "Epoch 13/30\n",
            "172/172 [==============================] - 67s 389ms/step - loss: 0.9021\n",
            "Epoch 14/30\n",
            "172/172 [==============================] - 67s 389ms/step - loss: 0.8272\n",
            "Epoch 15/30\n",
            "172/172 [==============================] - 67s 389ms/step - loss: 0.7493\n",
            "Epoch 16/30\n",
            "172/172 [==============================] - 67s 389ms/step - loss: 0.6706\n",
            "Epoch 17/30\n",
            "172/172 [==============================] - 67s 389ms/step - loss: 0.5983\n",
            "Epoch 18/30\n",
            "172/172 [==============================] - 67s 389ms/step - loss: 0.5303\n",
            "Epoch 19/30\n",
            "172/172 [==============================] - 67s 389ms/step - loss: 0.4707\n",
            "Epoch 20/30\n",
            "172/172 [==============================] - 67s 389ms/step - loss: 0.4206\n",
            "Epoch 21/30\n",
            "172/172 [==============================] - 67s 389ms/step - loss: 0.3760\n",
            "Epoch 22/30\n",
            "172/172 [==============================] - 67s 389ms/step - loss: 0.3397\n",
            "Epoch 23/30\n",
            "172/172 [==============================] - 67s 389ms/step - loss: 0.3116\n",
            "Epoch 24/30\n",
            "172/172 [==============================] - 67s 389ms/step - loss: 0.2899\n",
            "Epoch 25/30\n",
            "172/172 [==============================] - 67s 389ms/step - loss: 0.2734\n",
            "Epoch 26/30\n",
            "172/172 [==============================] - 67s 389ms/step - loss: 0.2612\n",
            "Epoch 27/30\n",
            "172/172 [==============================] - 67s 390ms/step - loss: 0.2491\n",
            "Epoch 28/30\n",
            "172/172 [==============================] - 67s 390ms/step - loss: 0.2409\n",
            "Epoch 29/30\n",
            "172/172 [==============================] - 67s 390ms/step - loss: 0.2372\n",
            "Epoch 30/30\n",
            "172/172 [==============================] - 67s 390ms/step - loss: 0.2347\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 30\n",
        "#  Increased number of epochs to improve the performance of the model.\n",
        "\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOXZUCpGkFfa"
      },
      "source": [
        "## Generate Texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a57X8y-Kbw-"
      },
      "source": [
        "Here we write a class to generate characters based on the model we trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "n1u0o-jvmezu"
      },
      "outputs": [],
      "source": [
        "class OneChar(tf.keras.Model):\n",
        "    def __init__(self, model, id2char, char2id, temperature=1.0):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.model = model\n",
        "        self.chars_from_ids = id2char\n",
        "        self.ids_from_chars = char2id\n",
        "\n",
        "        # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "        skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "        sparse_mask = tf.SparseTensor(\n",
        "            # Put a -inf at each bad index.\n",
        "            values=[-float('inf')]*len(skip_ids),\n",
        "            indices=skip_ids,\n",
        "            # Match the shape to the vocabulary\n",
        "            dense_shape= [vocab_size_]\n",
        "        )\n",
        "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "    @tf.function\n",
        "    def generate_one_char(self, inputs, states_1=None, states_2=None):\n",
        "        # Convert strings to token IDs.\n",
        "        input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "        # Convert tokens to ids\n",
        "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "        # Run the model and get the states and predicted logits\n",
        "        # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "        # predicted_logits, states  = self.model(inputs=input_ids, states=states, return_state=True)\n",
        "        predicted_logits, states_1, states_2  = self.model(inputs=input_ids, states_1=states_1, states_2=states_2, return_state=True)\n",
        "        # predicted_logits, memory_state, carry_state  = self.model(inputs=input_ids, states=states, return_state=True)\n",
        "        # states =  [memory_state, carry_state]  \n",
        "\n",
        "        # Only use the last prediction.\n",
        "        predicted_logits = predicted_logits[:, -1, :]\n",
        "        predicted_logits = predicted_logits/self.temperature\n",
        "        # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "        predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "        # Sample the output logits to generate token IDs (use random.categorical).\n",
        "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "        # Convert from token ids to characters\n",
        "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "        # Return the characters and model state.\n",
        "        return predicted_chars, states_1, states_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "UwdouIS9oXz8"
      },
      "outputs": [],
      "source": [
        "one_char_model = OneChar(model, id2char, char2id, temperature=0.8)\n",
        "# Used lower temperature for less random generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCxXBl0Moo0I"
      },
      "source": [
        "Run it in a loop to generate some text. Looking at the generated text, you'll see the model knows when to capitalize, make paragraphs and imitates a Shakespeare-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qxLixtYoohV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7946ec35-1aa3-4134-a853-db56a52f5499"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "I prithee: my most gracious lord.\n",
            "\n",
            "ABRAHAM:\n",
            "I do beauty tenger one.\n",
            "\n",
            "PAULINA:\n",
            "Good morrow, gentle Isabel!\n",
            "Seeet BerWaffully my uncle well undereadest,\n",
            "To think it still from Frontence ta'en.\n",
            "\n",
            "BAPTISTA:\n",
            "Well, girl, sir! no.\n",
            "\n",
            "MIRANDA:\n",
            "O virtue gall men thunder\n",
            "Than set how to bring these frown Katch'd and rivers\n",
            "Or worthy babe:' the cause of all the facred of himself\n",
            "and abroach 'gainst the heavens fond of the sea,\n",
            "And let me go and women air.\n",
            "\n",
            "BAPTISTA:\n",
            "Now, bride-a! which I wore my poor night!\n",
            "\n",
            "GrOMNO:\n",
            "God save your part, you look you, sir, ha?\n",
            "\n",
            "GONZALO:\n",
            "I am a puppet of home; come, I beseech you,\n",
            "Where be these confessor, unless ye need not\n",
            "where never after how to close you taught to speak,\n",
            "And now but sweeter than the sea for to sits of bawd.\n",
            "\n",
            "GRUMIO:\n",
            "Nay, tarry; I cannot think it would have married with me.\n",
            "\n",
            "PETRUCHIO:\n",
            "A juleare! I am no brother's unjust,\n",
            "Gentlemen, for in such a gently rid\n",
            "Who sinks nothing else would grown in a\n",
            "mouth: my raven with his maids!\n",
            "And like a whoreson \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 5.061949729919434\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "    # next_char,  memory_state, carry_state = one_char_model.generate_one_char(next_char, states=states)\n",
        "    next_char,  states = one_char_model.generate_one_char(next_char, states=states)\n",
        "    # states =  [memory_state, carry_state] \n",
        "\n",
        "    result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvmmtzvepHjI"
      },
      "source": [
        "Now play with the model and hyperparameters (epochs, ...) and run this again to see if the results have improved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "xsRiYKTLpU8H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a50f85d-0a57-45b5-f507-463cee584208"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Nay, not were worth as should have assured thee,\n",
            "And chief your table strew me. Thus far off from angry now\n",
            "Of the year of whom, they can make foul winds,\n",
            "Breathe without the devil's dam: conceives her there;\n",
            "Or bide thee, poor surfelon, to Belken his eyes\n",
            "Will have my rapies bear me here.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "That shall not be much amiss: Yet, as 'twere a brother\n",
            "As fresh and bear. O rack, for those all things should be\n",
            "unlikedies in the sun,\n",
            "And bloody dagger with him, he'll be my head.\n",
            "\n",
            "TRANIO:\n",
            "I love no chiders, sir. Bioneel, dispatch;\n",
            "Why story peace is a good friar Bianca craves you! hear you?\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "You have told me too malapett.\n",
            "\n",
            "LUCIO:\n",
            "\n",
            "ISABELLA:\n",
            "Thou liest. Is't not Hortensio?\n",
            "\n",
            "BIANCA:\n",
            "Hear, take the urgener hall! Go, this is well,\n",
            "And to my state grew so far that we would not do't,\n",
            "Let me embrace thee in himself.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Look where he comes.\n",
            "\n",
            "MENENIUS:\n",
            "Ay, if you call?\n",
            "\n",
            "POMPEY:\n",
            "Truly, sir, in my poor opinion, the mine own priest\n",
            "And promise them such ends now \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 7.9281532764434814\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states_1=None\n",
        "states_2=None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states_1, states_2 = one_char_model.generate_one_char(next_char, states_1=states_1, states_2=states_2)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "w3-zwob_fALQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "NLP_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}