{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### references and used links:\n",
    "\n",
    "https://www.w3schools.com/python/python_regex.asp\n",
    "\n",
    "https://www.regexpal.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a function with given inputs list\n",
    "def test_function(test_input, function):\n",
    "    for test in test_input:\n",
    "        result = function(test)\n",
    "        print(result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_email_validator(string):\n",
    "    pattern = \"^[\\w\\.]+@[a-zA-Z\\d]+\\.([a-zA-Z]){3}$\"\n",
    "    result = re.search(pattern, string)\n",
    "    return bool(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "emails = [\"username@domain.old\", \"username@@domain.old\", \"user_name@doma9in.old\", \"username@do.main.old\", \"username@domain.ookld\"]\n",
    "          \n",
    "test_function(emails, regex_email_validator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_number_validator(string):\n",
    "    pattern = \"^(0098|\\+98|0)9([\\d]){9}$\"\n",
    "    result = re.search(pattern, string)\n",
    "    return bool(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "texts = [\"+989143456789\", \"09143456789\", \"00989143456789\", \"+99143456780\", \"0122948328\", \"+98943456789\"]\n",
    "\n",
    "test_function(texts, regex_number_validator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.b.1 - Lemmatizer & Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the verb root in a dictionary\n",
    "def dictionary_check(string):\n",
    "    dictionary = {\"رو\" : \"رفت\"\n",
    "                  ,\"رفت\" : \"رفت\"\n",
    "                  ,\"گفت\" : \"گفت\"\n",
    "                  ,\"گوی\" : \"گفت\"\n",
    "                  ,\"خور\" : \"خورد\"\n",
    "                  ,\"خورد\" : \"خورد\"\n",
    "                  ,\"نوشت\" : \"نوشت\"\n",
    "                 }\n",
    "    if string in dictionary.keys():\n",
    "        return dictionary[string]\n",
    "    else:\n",
    "        return string\n",
    "    \n",
    "\n",
    "# Lemmatizer for one word\n",
    "def persian_lemmatizer_one_sample(string):\n",
    "#     prefix_list = [\"با\",\"بی\",\"بر\",\"فرا\",\"فرو\",\"نا\",\"هم\",\"می\"]\n",
    "#     suffix_list = [\"آسا\",\"ان\",\"انه\",\"تر\",\"ترین\",\"زار\",\"سان\",\"ستان\",\"گار\",\"وار\",\"ها\",\"ات\",\"اون\",\"ین\",\"م\",\"ش\",\"ت\",\"مان\",\"تان\",\"شان\",\"ه\"]\n",
    "    string = string.replace(\"\\u200c\", \"\")\n",
    "    \n",
    "    prefix_list = [\"می\"]\n",
    "    suffix_list = [\"ها\"\n",
    "                   ,\"ها\"\n",
    "                   ,\"ان\"\n",
    "                   ,\"ات\"\n",
    "                   ,\"هاند\"\n",
    "                   ,\"ه بودند\"\n",
    "                   ,\"ید\"\n",
    "                   ,\"م\"\n",
    "                   ,\"ند\"\n",
    "                   ,\"ید\"\n",
    "                   ,\"ه\"]\n",
    "    \n",
    "    exceptions = [\"بوستان\"]\n",
    "\n",
    "\n",
    "    if string in exceptions:\n",
    "        return string\n",
    "    else:\n",
    "        for prefix in prefix_list:\n",
    "            if string.startswith(prefix):\n",
    "                string = string[len(prefix):]\n",
    "                break\n",
    "        for suffix in suffix_list:\n",
    "            if string.endswith(suffix):\n",
    "                string = string[:-len(suffix)]\n",
    "                break\n",
    "        return dictionary_check(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'خورد'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persian_lemmatizer_one_sample('خوردم')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemmer for one word\n",
    "def persian_stemmer_one_sample(string):\n",
    "    string = string.replace(\"\\u200c\", \"\")\n",
    "    \n",
    "    prefix_list = [\"می\"]\n",
    "    suffix_list = [\"ها\",\"ان\",\"ات\",\" اند\",\"بودند\",\"ید\",\"م\",\"ند\",\"ید\"]\n",
    "    exceptions = [\"بوستان\"]\n",
    "    \n",
    "    if string in exceptions:\n",
    "        return string\n",
    "    else:\n",
    "        for prefix in prefix_list:\n",
    "            if string.startswith(prefix):\n",
    "                string = string[len(prefix):]\n",
    "                break\n",
    "        for suffix in suffix_list:\n",
    "            if string.endswith(suffix):\n",
    "                string = string[:-len(suffix)]\n",
    "                break\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'گفت'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persian_stemmer_one_sample('میگفتند')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemmer and Lemmatizer both together in one function\n",
    "def lemmatizer_stemmer_one_sample(string):\n",
    "    string = persian_stemmer_one_sample(string)\n",
    "    string = persian_lemmatizer_one_sample(string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'رفت'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer_stemmer_one_sample('رفته اند')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Tokenizer + \"،\" ignorer\n",
    "def tokenizer(sentence):\n",
    "    sentence = sentence.replace(\"،\", \"\")\n",
    "    return sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['سلام', 'من', 'رفتم']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"سلام، من رفتم\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put it all together for one sentence\n",
    "def tokenizer_lemmatizer_stemmer(sentence):\n",
    "    out = []\n",
    "    tokens = tokenizer(sentence)\n",
    "    for token in tokens:\n",
    "#         print(token)\n",
    "        out.append(lemmatizer_stemmer_one_sample(token))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'کتاب', 'گفت', 'نوشت', 'فردا', 'اتصال', '</s>']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"<s> کتاب\\u200cها، گفته\\u200cبودند، نوشتید، فردا، اتصالات </s>\"\n",
    "tokenizer_lemmatizer_stemmer(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.b.2 - Bi-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we create one-grams and its dictionary\n",
    "def onegrams_generator_for_one_sentence(sentence):\n",
    "    cleaned_tokens = tokenizer_lemmatizer_stemmer(sentence)\n",
    "    onegrams = []\n",
    "    for i in range(len(cleaned_tokens)):\n",
    "        onegrams.append(cleaned_tokens[i])\n",
    "        \n",
    "    return onegrams\n",
    "\n",
    "\n",
    "def add_one_sentence_into_onegram_dictionary(sentence, onegrams_dictionary):\n",
    "    onegrams = onegrams_generator_for_one_sentence(sentence)\n",
    "    for onegram in onegrams:\n",
    "        if onegram in onegrams_dictionary:\n",
    "            onegrams_dictionary[onegram] += 1\n",
    "        else:\n",
    "            onegrams_dictionary[onegram] = 1\n",
    "\n",
    "            \n",
    "def all_onegrams_dictionary_generator(sentences_list):\n",
    "    onegrams_dictionary = {}\n",
    "    for sentence in sentences_list:\n",
    "        add_one_sentence_into_onegram_dictionary(sentence, onegrams_dictionary)\n",
    "        \n",
    "    return onegrams_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test onegrams functions\n",
    "# onegrams = onegrams_generator_for_one_sentence(sentence)\n",
    "# onegrams_dictionary = all_onegrams_dictionary_generator([sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate bi-grams and its dictionary\n",
    "def bigrams_generator_for_one_sentence(sentence):\n",
    "    cleaned_tokens = tokenizer_lemmatizer_stemmer(sentence)\n",
    "    bigrams = []\n",
    "    for i in range(len(cleaned_tokens)-1):\n",
    "        bigrams.append(\"$$$$\".join([cleaned_tokens[i], cleaned_tokens[i+1]]))\n",
    "        \n",
    "    return bigrams\n",
    "\n",
    "def bigram_priner(bigrams):\n",
    "    for bigram in bigrams:\n",
    "        print(bigram.split(\"$$$$\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_one_sentence_into_bigram_dictionary(sentence, bigrams_dictionary):\n",
    "    bigrams = bigrams_generator_for_one_sentence(sentence)\n",
    "    for bigram in bigrams:\n",
    "        if bigram in bigrams_dictionary:\n",
    "            bigrams_dictionary[bigram] += 1\n",
    "        else:\n",
    "            bigrams_dictionary[bigram] = 1\n",
    "\n",
    "def all_bigrams_dictionary_generator(sentences_list):\n",
    "    bigrams_dictionary = {}\n",
    "    for sentence in sentences_list:\n",
    "        add_one_sentence_into_bigram_dictionary(sentence, bigrams_dictionary)\n",
    "        \n",
    "    return bigrams_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test bi-grams functions\n",
    "# bigrams = bigrams_generator_for_one_sentence(sentence)\n",
    "# bigram_priner(bigrams)\n",
    "# bigrams_dictionary = all_bigrams_dictionary_generator([sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate one-grams and bi-grams dictionary for trainset:\n",
    "def onegrams_and_bigrams_dictionary_generator(sentences_list):\n",
    "    bigrams_dictionary = all_bigrams_dictionary_generator(sentences_list)\n",
    "    onegrams_dictionary = all_onegrams_dictionary_generator(sentences_list)\n",
    "    return onegrams_dictionary, bigrams_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = [\n",
    "    \"<s> کتاب\\u200cها، گفته\\u200cبودند، نوشتید، فردا، اتصالات </s>\",\n",
    "    \"<s> کتاب\\u200cها، آن\\u200cها، درختان، می\\u200cروم، می\\u200cگویید </s>\",\n",
    "    \"<s> می\\u200cخورم، می\\u200cگویید، نوشتید، اتصالات </s>\",\n",
    "    \"<s> آن\\u200cها، درختان، رفته\\u200cاند، کتاب\\u200cها، اتصالات </s>\",\n",
    "    \"<s> کتاب\\u200cها، رفته\\u200cاند، می\\u200cخورم، نوشتید </s>\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "onegrams_dictionary, bigrams_dictionary = onegrams_and_bigrams_dictionary_generator(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary_printer(dictionary, bi=False):\n",
    "    for key,value in dictionary.items():\n",
    "        if bi:\n",
    "            print(tuple((key.split(\"$$$$\"), value)))\n",
    "        else:\n",
    "            print(tuple((key,value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<s>', 5)\n",
      "('کتاب', 4)\n",
      "('گفت', 3)\n",
      "('نوشت', 3)\n",
      "('فردا', 1)\n",
      "('اتصال', 3)\n",
      "('</s>', 5)\n",
      "('آن', 2)\n",
      "('درخت', 2)\n",
      "('رفت', 3)\n",
      "('خورد', 2)\n"
     ]
    }
   ],
   "source": [
    "dictionary_printer(onegrams_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['<s>', 'کتاب'], 3)\n",
      "(['کتاب', 'گفت'], 1)\n",
      "(['گفت', 'نوشت'], 2)\n",
      "(['نوشت', 'فردا'], 1)\n",
      "(['فردا', 'اتصال'], 1)\n",
      "(['اتصال', '</s>'], 3)\n",
      "(['کتاب', 'آن'], 1)\n",
      "(['آن', 'درخت'], 2)\n",
      "(['درخت', 'رفت'], 2)\n",
      "(['رفت', 'گفت'], 1)\n",
      "(['گفت', '</s>'], 1)\n",
      "(['<s>', 'خورد'], 1)\n",
      "(['خورد', 'گفت'], 1)\n",
      "(['نوشت', 'اتصال'], 1)\n",
      "(['<s>', 'آن'], 1)\n",
      "(['رفت', 'کتاب'], 1)\n",
      "(['کتاب', 'اتصال'], 1)\n",
      "(['کتاب', 'رفت'], 1)\n",
      "(['رفت', 'خورد'], 1)\n",
      "(['خورد', 'نوشت'], 1)\n",
      "(['نوشت', '</s>'], 1)\n"
     ]
    }
   ],
   "source": [
    "dictionary_printer(bigrams_dictionary, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate p for one bigram with Laplace smoothing\n",
    "def one_bigram_probability_calculator(bigram, onegrams_dictionary, bigrams_dictionary, log):\n",
    "    [first,second] = bigram.split(\"$$$$\")\n",
    "    V = len(onegrams_dictionary)-1\n",
    "    \n",
    "    if bigram in bigrams_dictionary:\n",
    "        count_both = bigrams_dictionary[bigram]\n",
    "    else:\n",
    "        count_both = 0\n",
    "        \n",
    "    if first in onegrams_dictionary:\n",
    "        count_first = onegrams_dictionary[first]\n",
    "    else:\n",
    "        count_first = 0  \n",
    "        \n",
    "    p = (count_both + 1)/(count_first + V)\n",
    "    if log:\n",
    "        print(\"count_both: \",count_both, \"count_first: \",count_first, \"V: \", V, \"p: \", p)\n",
    "\n",
    "    return p\n",
    "\n",
    "        \n",
    "# Calculate p of one sentence by multiplying all the bigrams one\n",
    "def one_sentence_probability_calculator(sentence, onegrams_dictionary, bigrams_dictionary, log=False):\n",
    "    bigrams = bigrams_generator_for_one_sentence(sentence)\n",
    "    probabilities_list = []\n",
    "    for bigram in bigrams:\n",
    "        probabilities_list.append(one_bigram_probability_calculator(bigram, onegrams_dictionary, bigrams_dictionary, log))\n",
    "        \n",
    "    final_probability = 1\n",
    "    for p in probabilities_list:\n",
    "        final_probability *= p\n",
    "        \n",
    "    return final_probability\n",
    "\n",
    "\n",
    "# Final function to perform on a test set - also generate the dictionaries by receiving the train set as an input\n",
    "def test_set_probability_calculator(test_set, train_set, log=False):\n",
    "    onegrams_dictionary, bigrams_dictionary = onegrams_and_bigrams_dictionary_generator(train_set)\n",
    "    \n",
    "    for i in range(len(test_set)):\n",
    "        p = one_sentence_probability_calculator(test_set[i], onegrams_dictionary, bigrams_dictionary, log)\n",
    "        print(\"Probability of sentence number {} in the test set is {}\".format(i, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = [\n",
    "    \"<s> کتاب\\u200cها، می\\u200cگویید، می\\u200cخورم، بوستان، اتصالات </s>\",\n",
    "    \"<s> بوستان، رفته\\u200cاند، کتاب\\u200cها، می\\u200cگویید، می\\u200cگویید </s>\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are the details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_both:  3 count_first:  5 V:  10 p:  0.26666666666666666\n",
      "count_both:  1 count_first:  4 V:  10 p:  0.14285714285714285\n",
      "count_both:  0 count_first:  3 V:  10 p:  0.07692307692307693\n",
      "count_both:  0 count_first:  2 V:  10 p:  0.08333333333333333\n",
      "count_both:  0 count_first:  0 V:  10 p:  0.1\n",
      "count_both:  3 count_first:  3 V:  10 p:  0.3076923076923077\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7.513853667699822e-06"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_sentence_probability_calculator(test_set[0], onegrams_dictionary, bigrams_dictionary, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_both:  0 count_first:  5 V:  10 p:  0.06666666666666667\n",
      "count_both:  0 count_first:  0 V:  10 p:  0.1\n",
      "count_both:  1 count_first:  3 V:  10 p:  0.15384615384615385\n",
      "count_both:  1 count_first:  4 V:  10 p:  0.14285714285714285\n",
      "count_both:  0 count_first:  3 V:  10 p:  0.07692307692307693\n",
      "count_both:  1 count_first:  3 V:  10 p:  0.15384615384615385\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.7339662310076517e-06"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_sentence_probability_calculator(test_set[1], onegrams_dictionary, bigrams_dictionary,  log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally we test all in one function wiht final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of sentence number 0 in the test set is 7.513853667699822e-06\n",
      "Probability of sentence number 1 in the test set is 1.7339662310076517e-06\n"
     ]
    }
   ],
   "source": [
    "test_set_probability_calculator(test_set, train_set, log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
